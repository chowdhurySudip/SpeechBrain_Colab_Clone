{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRTJCdy-Y6pw"
      },
      "source": [
        "# Complex and Quaternion Neural Networks with SpeechBrain\n",
        "\n",
        "This tutorial demonstrates how to use the SpeechBrain implementation of complex-valued and quaternion-valued neural networks for speech technologies. It covers the basics of highdimensional representations and the associated neural layers : Linear, Convolution, Recurrent and Normalisation. \n",
        "\n",
        "## Prerequisites\n",
        "- [SpeechBrain Introduction](https://colab.research.google.com/drive/12bg3aUdr9mTfOGqcB5pSMABoIKPgiwcM?usp=sharing)\n",
        "- [YAML tutorial](https://colab.research.google.com/drive/1Pg9by4b6-8QD2iC0U7Ic3Vxq4GEwEdDz?usp=sharing)\n",
        "- [Brain Class tutorial](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing)\n",
        "- [Speech Features tutorial](https://colab.research.google.com/drive/1CI72Xyay80mmmagfLaIIeRoDgswWHT_g?usp=sharing)\n",
        "\n",
        "## Introduction and background\n",
        "\n",
        "Complex-numbers are an extension or real numbers to the two dimensional space, they are composed of two parts : real and imaginary. A real number `z` is often expressed as:`z = r + ix`. Complex numbers are used in a wide variety of real-world applications as they provide an adequate algebra to manipulate concepts in the two dimensional space (i.e. rotations, translations, phase ...). As a matter of facts, complex-numbers offer a natural representation to the speech signal. For instance, the well-known Fourier transform relies on a weighted sum of complex sinusoids with increasing frequency, hence its output is defined in the complex space (i.e. amplitude and phase). Sometimes, the phase information is discarded to allow further processing in the real-valued space. However, many applications including speaker related ones could benefit from the entire complex-valued representation. \n",
        "\n",
        "Quaternion-numbers, on the other hands, are a generalisation of complex-numbers to the three dimensional space. They also contain a real (`r`) and an imaginary part but the latter element is in fact a 3D vector (`ix + jy + kz`). A quaternion `q` can be expressed as: `q = r + ix + jy + kz`. In practice, a quaternion defines a 3D rotation. Quaternions are extremely useful in physics, computer science, computer graphics or in the robotic domain such as for kinematic. Indeed, they provide a stable, natural and smooth way of conceiving and interpreting movements in our 3D space. \n",
        "\n",
        "### How do they connect to neural networks?\n",
        "\n",
        "As soon as the modern deep learning resurgence started, researchers tried to replace the traditional real algebra with complex and quaternion numbers to fit specific tasks. As an example, complex-valued neural networks (CVNN) could be used to directly deal with the FFT output while quaternion neural networks (QNN) could be implemented to generated realistic robot movements. \n",
        "\n",
        "Besides the natural fit to some representations that QNN and CVNN offer, they also share an interesting property: **weight sharing**. Indeed, quaternion and complex numbers follow a specific algebra, with well-defined rules that must be translated into the developped Q-CVNN. In particular, one does not multiply to quaternions or two complex-numbers in the same manner as two real numbers. Hence, all the dot product existing in real-valued numbers are replaced with the corresponding complex and Hamilton products finally inducing a mechanism of **weight sharing**. The latter mechanism has been demonstrated to be extremely usefull to learn expressive representation of multidimensional inputs while preserving the internal relation that exist within the components of the signal (e.g amplitude and phase for complex numbers). \n",
        "\n",
        "In this tutorial, we won't go into the details of all these properties as it would be way too long. Instead, we propose to detail how to use such CVNN and QNN with SpeechBrain.\n",
        "\n",
        "### Relevant bibliography\n",
        "- *Andreescu, T., & Andrica, D. (2006). Complex Numbers from A to... Z (Vol. 165). Boston: Birkhäuser.*\n",
        "- *Altmann, S. L. (1989). Hamilton, Rodrigues, and the quaternion scandal. Mathematics Magazine, 62(5), 291-308.*\n",
        "- **Complex Neural Networks Survey:** *Hirose, A. (2012). Complex-valued neural networks (Vol. 400). Springer Science & Business Media.*\n",
        "- **All about Quaternion Neural Networks:** *Parcollet, T., (2019) Quaternion Neural Networks, PhD Thesis, Avignon Université* \n",
        "\n",
        "# SpeechBrain representation of Complex and Quaternions\n",
        "\n",
        "Within SpeechBrain algebra operations are abstracted in the neural layers. This allows our users to not focus on the initial representation. In practice, it means that one does not have to declare a specific type of Tensor to use quaternion or complex numbers. More precisely, we will always manipulate real-valued Tensors. Indeed, all the operations corresponding to the different algebras can be expressed in a Tensor / matrix format, enabling an easy integration with modern GPU. \n",
        "\n",
        "Let's get practical: Any PyTorch Tensor generated in your recipe can be viewed as a complex or quaternion-valued Tensor. Indeed, it depends on the layer that processes it. If it's a `torch.nn.Linear`, then it will be real. If it's a `nnet.complex_networks.c_linear.CLinear` then it will be complex! \n",
        "\n",
        "**Wait, how do you interpret and build my tensors then?**\n",
        "\n",
        "Simple, let's say we want to consider a Tensor made of `3` complex numbers or `3` quaternions. The different parts of the numbers will simple be concatenated in the following way:\n",
        "\n",
        "`c_tensor = [r,r,r,x,x,x] and q_tensor = [r,r,r,x,x,x,y,y,y,z,z,z]`\n",
        "\n",
        "This is the reason why any Tensor you declare can be seen as a complex or a quaternion Tensor for a {C/Q}-Layer in SpeechBrain, as long as the features dimension can be divided by 2 for complex and 4 for quaternion numbers. \n",
        "\n",
        "Now, we need to install SpeechBrain to better illustrate this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pyD-gKql_qF"
      },
      "source": [
        "%%capture\n",
        "# Install from PyPI\n",
        "!pip install speechbrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5OjwamKmFNw"
      },
      "source": [
        "Alternatively, you can also clone the repository to have access to all the recipes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpG4LHHbXWHq"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDizGKe3mds9"
      },
      "source": [
        "Now, let's try to manipulate some Tensor to better understand the formalism. We start by instantiating a Tensor containing 8 real numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVQsQ3CKm-_d",
        "outputId": "d7868000-6947-43bb-fc85-f9059a297f8f"
      },
      "source": [
        "import torch\n",
        "\n",
        "T = torch.rand((1,8))\n",
        "print(T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3591, 0.6284, 0.9070, 0.7837, 0.5425, 0.0614, 0.3884, 0.8479]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3nGWQH1nKJT"
      },
      "source": [
        "Then, we access the SpeechBrain libary for manipulating complex numbers and we simply display the different parts (real, imaginary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G72PTJ7bnYMm",
        "outputId": "74d1df51-ce53-4974-b901-03f7b30ae5d5"
      },
      "source": [
        "from speechbrain.nnet.complex_networks.c_ops import get_real, get_imag\n",
        "\n",
        "print(get_real(T))\n",
        "print(get_imag(T))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3591, 0.6284, 0.9070, 0.7837]])\n",
            "tensor([[0.5425, 0.0614, 0.3884, 0.8479]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJdNeYasoN7w"
      },
      "source": [
        "As you can see, the initial Tensor is simply splitted in 2 and the same happens with 4 and quaternions.\n",
        "\n",
        "# Complex and quaternion products\n",
        "\n",
        "At the core of QNN and CVNN is the product. Of course, others specificities exist such as the weight initialisation, specific normalisations, activation functions etc. Nevertheless, the basic product is central to all neural network layers : a weight matrix that multplies the input vector. \n",
        "\n",
        "A very good thing to know is that a complex number can be represented in a real-valued matrix format:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{rr}\n",
        "a & -b \\\\\n",
        "b & a\n",
        "\\end{array}\\right).\n",
        "\\end{equation}\n",
        "\n",
        "The same goes for a quaternion number:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{cccc}\n",
        "a & -b & -c & -d \\\\\n",
        "b & a & -d & c \\\\\n",
        "c & d & a & -b \\\\\n",
        "d & -c & b & a\n",
        "\\end{array}\\right).\n",
        "\\end{equation}\n",
        "\n",
        "And even more interestingly, if we multiply two of these matrices, then we obtain the product corresponding to the considered algebra. For instance, the complex product between two complex number is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{rr}\n",
        "a & -b \\\\\n",
        "b & a\n",
        "\\end{array}\\right)\\left(\\begin{array}{lr}\n",
        "c & -d \\\\\n",
        "d & c\n",
        "\\end{array}\\right)=\\left(\\begin{array}{cc}\n",
        "a c-b d & -a d-b c \\\\\n",
        "b c+a d & -b d+a c\n",
        "\\end{array}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "which is equivalent to the formal definition:\n",
        "\n",
        "\\begin{equation}\n",
        "(a+\\mathrm{i} b)(c+\\mathrm{i} d)=(a c-b d)+\\mathrm{i}(a d+b c).\n",
        "\\end{equation}\n",
        "\n",
        "**Ok, so how is this implemented in SpeechBrain**?\n",
        "\n",
        "Every single layer that you can call either on the complex or quaternion libraries will follow two steps:\n",
        "1. *init()*: Define the complex / quaternion weights as torch.Parameters and initialise them with the adapted scheme.\n",
        "2. *forward()*: Call the corresponding operation that implements the specific product. For instance, a complex linear layer would call the `complex_linear_op()` from `speechbrain.nnet.complex_networks.c_ops`.\n",
        "\n",
        "In practice, the `speechbrain.nnet.complex_networks.c_ops.complex_linear_op` function simply:\n",
        "1. Takes the weights of the layer and builds the corresponding real-valued matrix.\n",
        "2. Apply a product between the input and this matrix to simulate the complex / quaternion products.\n",
        "\n",
        "Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALubUSk47CAT",
        "outputId": "0fb65684-0ed3-485c-8e38-c2847e0be1a2"
      },
      "source": [
        "def complex_linear_op(input, real_weight, imag_weight, bias):\n",
        "    \"\"\"\n",
        "    Applies a complex linear transformation to the incoming data.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input : torch.Tensor\n",
        "        Complex input tensor to be transformed.\n",
        "    real_weight : torch.Parameter\n",
        "        Real part of the quaternion weight matrix of this layer.\n",
        "    imag_weight : torch.Parameter\n",
        "        First imaginary part of the quaternion weight matrix of this layer.\n",
        "    bias : torch.Parameter\n",
        "    \"\"\"\n",
        "\n",
        "    # Here we build the real-valued matrix as defined by the equations!\n",
        "    cat_real = torch.cat([real_weight, -imag_weight], dim=0)\n",
        "    cat_imag = torch.cat([imag_weight, real_weight], dim=0)\n",
        "    cat_complex = torch.cat([cat_real, cat_imag], dim=1)\n",
        "\n",
        "    # If the input is already [batch*time, N]\n",
        "\n",
        "    # We do inputxconstructed_matrix to simulate the product\n",
        "\n",
        "    if input.dim() == 2:\n",
        "        if bias.requires_grad:\n",
        "            return torch.addmm(bias, input, cat_complex)\n",
        "        else:\n",
        "            return torch.mm(input, cat_complex)\n",
        "    else:\n",
        "        output = torch.matmul(input, cat_complex)\n",
        "        if bias.requires_grad:\n",
        "            return output + bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "# We create a single complex number\n",
        "complex_input = torch.rand(1, 2)\n",
        "\n",
        "# We create two Tensors (not parameters here because we don't care about storing gradients)\n",
        "# These tensors are the real_parts and imaginary_parts of the weight matrix.\n",
        "# The real part is equivalent [nb_complex_numbers_in // 2, nb_complex_numbers_out // 2]\n",
        "# The imag part is equivalent [nb_complex_numbers_in // 2, nb_complex_numbers_out // 2]\n",
        "# Hence if we define a layer with 1 complex input and 2 complex outputs:\n",
        "r_weight = torch.rand((1,2))\n",
        "i_weight = torch.rand((1,2))\n",
        "\n",
        "bias = torch.ones(4) # because we have 2 (complex) x times 2 = 4 real-values\n",
        "\n",
        "# and we forward propagate!\n",
        "print(complex_linear_op(complex_input, r_weight, i_weight, bias).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-eaHzWP8m9R"
      },
      "source": [
        "**It is important to note that the quaternion implementation follows exactly the same approach.**\n",
        "\n",
        "# Complex-valued Neural Networks\n",
        "\n",
        "Once you are familiar with the formalism, you can easily derive any complex-valued neural building blocks given in `speechbrain.nnet.complex_networks`:\n",
        "- 1D and 2D convolutions.\n",
        "- Batch and layer normalisations.\n",
        "- Linear layers.\n",
        "- Recurrent cells (LSTM, LiGRU, RNN).\n",
        "\n",
        "*According to the litterature, most of the complex and quaternion neural networks rely on split activation functions (any real-valued activation function applied over the complex/quaternion valued signal). For now, SpeechBrain follows this approach and does not offer any fully complex or quaternion activation function*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_zMI-C7F5v"
      },
      "source": [
        "## Convolution layers\n",
        "\n",
        "First, let's define a batch of inputs (that could be the output of the FFT for example). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_yW5_jspf-O",
        "outputId": "fe6729e0-11a2-4a66-a32c-581daeb9e0aa"
      },
      "source": [
        "from speechbrain.nnet.complex_networks.c_CNN import CConv1d, CConv2d\n",
        "\n",
        "# [batch, time, features]\n",
        "T = torch.rand((8, 10, 32))\n",
        "\n",
        "# We define our layer and we want 12 complex numbers as output.\n",
        "cnn_1d = CConv1d( input_shape=T.shape, out_channels=12, kernel_size=3)\n",
        "\n",
        "out_tensor = cnn_1d(T)\n",
        "print(out_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 10, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a67ZqJs-qSU3"
      },
      "source": [
        "As we can see, we applied a Complex-Valued 1D convolution over the input Tensor and we obtained an output Tensor whose features dimension is equal to 24. Indeed, we requested 12 `out_channels` which is equivalent to 24 real-values. Remember : **we always work with real numbers, the algebra is abstracted in the layer itself!**\n",
        "\n",
        "The same can be done with 2D convolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgJE0QQTqo0J",
        "outputId": "ecd64f27-2d4a-4ec0-a001-f8605a4aab0a"
      },
      "source": [
        "# [batch, time, fea, Channel]\n",
        "T = torch.rand([10, 16, 30, 30])\n",
        "\n",
        "cnn_2d = CConv2d( input_shape=T.shape, out_channels=12, kernel_size=3)\n",
        "\n",
        "out_tensor = cnn_2d(T)\n",
        "print(out_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 16, 30, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z65CK8DNraT-"
      },
      "source": [
        "Please note that the 2D convolution is applied over the time and fea axis. The channel axis is used to be considered as the real and imaginary parts: `[10, 16, 30, 0:15] = real` and `[10, 16, 30, 15:30] = imag`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15YS16o59JT6"
      },
      "source": [
        "## Linear layer\n",
        "\n",
        "In the same manner as for convolution layers, we just need to instantiate the right module and use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zpY9fHj99I2",
        "outputId": "0501b65d-c382-4892-c64a-69126d84802c"
      },
      "source": [
        "from speechbrain.nnet.complex_networks.c_linear import CLinear\n",
        "\n",
        "# [batch, time, features]\n",
        "T = torch.rand((8, 10, 32))\n",
        "\n",
        "# We define our layer and we want 12 complex numbers as output.\n",
        "lin = CLinear(12, input_shape=T.shape, init_criterion='glorot', weight_init='complex')\n",
        "\n",
        "out_tensor = lin(T)\n",
        "print(out_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 10, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnE2n50p9_P4"
      },
      "source": [
        "Please notice that we added the `init_criterion` and `weight_init` arguments. These two parameters that exist in **ALL** the complex and quaternion layers define how the weights are initialised. Indeed, complex and quaternion-valued weights need a carefull initialisation process as detailled in *Deep Complex Networks* by Chiheb Trabelsy et al. and `Quaternion Recurrent Neural Networks` from Titouan Parcollet et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrGfOb4n-tsh"
      },
      "source": [
        "## Normalisation layers\n",
        "\n",
        "One do not normalise a set of complex numbers (e.g the output of a complex-valued layers) in the same manner as a set of real-valued numbers. Due to the complexity of the task, this tutorial won't go into the details. Please note that the code is fully available in the corresponding SpeechBrain library and that it strictly follows the description first made in the paper *Deep Complex Networks* by Chiheb Trabelsy et al. \n",
        "\n",
        "SpeechBrain supports both complex batch and layer normalisations:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCe-dYJc_VH0"
      },
      "source": [
        "from speechbrain.nnet.complex_networks.c_normalization import CBatchNorm,CLayerNorm\n",
        "\n",
        "inp_tensor = torch.rand([10, 16, 30])\n",
        "\n",
        "# Not that by default the complex axis is the last one, but it can be specified.\n",
        "CBN = CBatchNorm(input_shape=inp_tensor.shape)\n",
        "CLN = CLayerNorm(input_shape=inp_tensor.shape)\n",
        "\n",
        "out_bn_tensor = CBN(inp_tensor)\n",
        "out_ln_tensor = CLN(inp_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmx2EsrtAC8E"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "Recurrent neural cells are nothing more than multiple linear layers with a time connection. Hence, SpeechBrain provides an implementation for the complex variation of LSTM, RNN and LiGRU. As a matter of fact, these models are strictly equivalent to the real-valued ones, except that Linear layers are replaced with CLinear layers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxC24HR3AeGO",
        "outputId": "eec2bb5b-aacf-4ee5-c3b1-a2abc7b0cddd"
      },
      "source": [
        "from speechbrain.nnet.complex_networks.c_RNN import CLiGRU, CLSTM, CRNN\n",
        "\n",
        "inp_tensor = torch.rand([10, 16, 40])\n",
        "\n",
        "lstm = CLSTM(hidden_size=12, input_shape=inp_tensor.shape, weight_init='complex', bidirectional=True)\n",
        "rnn = CRNN(hidden_size=12, input_shape=inp_tensor.shape, weight_init='complex', bidirectional=True)\n",
        "ligru = CLiGRU(hidden_size=12, input_shape=inp_tensor.shape, weight_init='complex', bidirectional=True)\n",
        "\n",
        "print(lstm(inp_tensor).shape)\n",
        "print(rnn(inp_tensor).shape)\n",
        "print(ligru(inp_tensor).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 16, 48])\n",
            "torch.Size([10, 16, 48])\n",
            "torch.Size([10, 16, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bot3cQpGBSbw"
      },
      "source": [
        "Note that the output dimension is 48 as we have 12 complex numbers (24 values) times 2 directions (bidirectional RNNs).\n",
        "\n",
        "# Quaternion Neural Networks\n",
        "\n",
        "Luckily, QNN within SpeechBrain follow exactly the same formalism. Therefore, you can easily derive any quaternion-valued neural networks from the building blocks given in `speechbrain.nnet.quaternion_networks`:\n",
        "- 1D and 2D convolutions.\n",
        "- Batch and layer normalisations.\n",
        "- Linear and Spinor layers.\n",
        "- Recurrent cells (LSTM, LiGRU, RNN).\n",
        "\n",
        "*According to the litterature, most of the complex and quaternion neural networks rely on split activation functions (any real-valued activation function applied over the complex/quaternion valued signal). For now, SpeechBrain follows this approach and does not offer any fully complex or quaternion activation function*.\n",
        "\n",
        "Everything we just saw with complex neural networks still hold. Hence we can summarize everything in a single code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovxi7PdaCf5r",
        "outputId": "ffd431a6-1f84-4340-a565-d81638349c7b"
      },
      "source": [
        "from speechbrain.nnet.quaternion_networks.q_CNN import QConv1d, QConv2d\n",
        "from speechbrain.nnet.quaternion_networks.q_linear import QLinear\n",
        "from speechbrain.nnet.quaternion_networks.q_RNN import QLiGRU, QLSTM, QRNN\n",
        "\n",
        "# [batch, time, features]\n",
        "T = torch.rand((8, 10, 40))\n",
        "\n",
        "# [batch, time, fea, Channel]\n",
        "T_4d = torch.rand([10, 16, 30, 40])\n",
        "\n",
        "# We define our layers and we want 12 quaternion numbers as output (12x4 = 48 output real-values).\n",
        "cnn_1d = QConv1d( input_shape=T.shape, out_channels=12, kernel_size=3)\n",
        "cnn_2d = QConv2d( input_shape=T_4d.shape, out_channels=12, kernel_size=3)\n",
        "\n",
        "lin = QLinear(12, input_shape=T.shape, init_criterion='glorot', weight_init='quaternion')\n",
        "\n",
        "lstm = QLSTM(hidden_size=12, input_shape=T.shape, weight_init='quaternion', bidirectional=True)\n",
        "rnn = QRNN(hidden_size=12, input_shape=T.shape, weight_init='quaternion', bidirectional=True)\n",
        "ligru = QLiGRU(hidden_size=12, input_shape=T.shape, weight_init='quaternion', bidirectional=True)\n",
        "\n",
        "print(cnn_1d(T).shape)\n",
        "print(cnn_2d(T_4d).shape)\n",
        "print(lin(T).shape)\n",
        "print(lstm(T)[0].shape) # RNNs return output + hidden so we need to filter !\n",
        "print(ligru(T)[0].shape) # RNNs return output + hidden so we need to filter !\n",
        "print(rnn(T)[0].shape) # RNNs return output + hidden so we need to filter !\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 10, 48])\n",
            "torch.Size([10, 16, 30, 48])\n",
            "torch.Size([8, 10, 48])\n",
            "torch.Size([8, 10, 96])\n",
            "torch.Size([8, 10, 96])\n",
            "torch.Size([8, 10, 96])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q75NZOMhEFv2"
      },
      "source": [
        "## Quaternion Spinor Neural Networks\n",
        "\n",
        "Spinor neural networks are a special kind of quaternion-valued NNs. As stated above, quaternions have been invented to represent rotations. In the QNN layers defined above, the basic operation is `inputs x weights` with inputs and weights two set of quaternions and `x` the Hamilton product. \n",
        "\n",
        "In fact, multiplying two quaternions is equivalent to creating a new rotation that is a composition of the first rotation followed by the second one. For example: `q3 = q1 x q2` is equal to: *q3 is a rotation that is equivalent to a rotation by q1 followed by a rotation from q2*. **In this context, we aren't rotating objects, but we are composing new rotations**. Let's say you want to predict the next movement that a robot will do. In this particular case, you can use this concept to give your NN a quaternion as input (i.e. the previous movement) to produce a new quaternion as output (i.e next movement). \n",
        "\n",
        "Spinor Neural Networks (SNN) have been proposed to specifically model rotations. If we take the same robot example, our input would be the 3D coordinate (x,y,z) of the robot arm before the movement, while the output of the SNN would be its predicted coordinates after moving!\n",
        "\n",
        "To do so, we need to replace the standard product performed in all layers with:\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{v_{output}}=q_{weight} \\vec{v_{input}} q^{-1}_{weight}.\n",
        "\\end{equation}\n",
        "\n",
        "This equation is the formal definition of the rotation of a vector $\\vec{v}$ by a unit quaternion $q_{weight}$ (whose norm is equal to 1) with $q^{-1}$ its conjugate. Both left and right products are Hamilton product.\n",
        "\n",
        "**Ok, so how is this implemented in SpeechBrain?**\n",
        "\n",
        "In the exact same manner than for the standard Hamilton product! Indeed, such rotation can also be represented as a matrix product:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{ccc}\n",
        "a^{2}+b^{2}-c^{2}-d^{2} & 2 b c-2 a d & 2 a c+2 b d \\\\\n",
        "2 a d+2 b c & a^{2}-b^{2}+c^{2}-d^{2} & 2 c d-2 a b \\\\\n",
        "2 b d-2 a c & 2 a b+2 c d & a^{2}-b^{2}-c^{2}+d^{2}\n",
        "\\end{array}\\right).\n",
        "\\end{equation}\n",
        "\n",
        "Hence, we just need to define the `quaternion_op` that follows the same usual process: \n",
        "1. Compose a real-valued matrix from the different weight components\n",
        "2. Apply a matrix product between the input and this rotation matrix!\n",
        "\n",
        "[Check the code!](http://www.darnault-parcollet.fr/Parcollet/hiddennoshare/speechbrain.github.io/documentation/speechbrain.nnet.quaternion_networks.q_ops.html#speechbrain.nnet.quaternion_networks.q_ops.quaternion_linear_rotation_op)\n",
        "\n",
        "## Turning a quaternion layer into a spinor layer\n",
        "\n",
        "Spinor layer can be activated with a boolean parameter in all quaternion layers.\n",
        "Here are a couple of examples:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oioMgs8eIe3K",
        "outputId": "71b5ea06-48b6-47ec-c86a-a45ce35f490e"
      },
      "source": [
        "from speechbrain.nnet.quaternion_networks.q_CNN import QConv1d\n",
        "from speechbrain.nnet.quaternion_networks.q_linear import QLinear\n",
        "\n",
        "# [batch, time, features]\n",
        "T = torch.rand((8, 80, 16))\n",
        "\n",
        "#\n",
        "# NOTE: in this case the real components must be zero as spinor neural networks\n",
        "# only input and output 3D vectors ! We don't do it here for the sake of compactness\n",
        "#\n",
        "\n",
        "# We define our layers and we want 12 quaternion numbers as output (12x4 = 48 output real-values).\n",
        "cnn_1d = QConv1d( input_shape=T.shape, out_channels=12, kernel_size=3, spinor=True, vector_scale=True)\n",
        "lin = QLinear(12, input_shape=T.shape, spinor=True, vector_scale=True)\n",
        "\n",
        "print(cnn_1d(T).shape)\n",
        "print(lin(T).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 80, 48])\n",
            "torch.Size([8, 80, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVZm7xqFLkPF"
      },
      "source": [
        "Two remarks on Spinor layers:\n",
        "1. We need to set a vector_scale to train deep models. The vector scale is just an other set torch.Parameters that will scale down the output of each Spinor layers. Indeed, the output of a SNN layer is a set of 3D vectors that are the sum of rotated 3D vectors. Quaternion rotations do not affect the magnitude of the rotated vector. Hence, by summing over and over rotated 3D vectors, we might end up very quickly with very large values (i.e the training will explode).\n",
        "2. You might consider to use `weight_init='unitary'`. Indeed, quaternion rotations are valid only if the considered quaternion is unitary. Therefore, starting with unitary weights may facilitate the learning phase! \n",
        "\n",
        "# Putting everyting together!\n",
        "\n",
        "We provide a minimal example for both complex and quaternion neural networks:\n",
        "- `speechbrain/recipes/minimal_examples/neural_networks/ASR_CTC/hyperparams_complex_net.yaml`.\n",
        "- `speechbrain/recipes/minimal_examples/neural_networks/ASR_CTC/hyperparams_quaternion_net.yaml`.\n",
        "\n",
        "If we take a look at one of these YAML params file, we can easily distinguish how to build our model out of the different blocks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGU7t6oeM2ox"
      },
      "source": [
        "yaml_params = \"\"\" \n",
        "model: !new:speechbrain.nnet.containers.Sequential\n",
        "    input_shape: [!ref <N_batch>, null, 660]  # input_size\n",
        "    conv1: !name:speechbrain.nnet.quaternion_networks.q_CNN.QConv1d\n",
        "        out_channels: 16\n",
        "        kernel_size: 3\n",
        "    act1: !ref <activation>\n",
        "    conv2: !name:speechbrain.nnet.quaternion_networks.q_CNN.QConv1d\n",
        "        out_channels: 32\n",
        "        kernel_size: 3\n",
        "    nrm2: !name:speechbrain.nnet.quaternion_networks.q_CNN.QConv1d\n",
        "    act2: !ref <activation>\n",
        "    pooling: !new:speechbrain.nnet.pooling.Pooling1d\n",
        "        pool_type: \"avg\"\n",
        "        kernel_size: 3\n",
        "    RNN: !name:speechbrain.nnet.quaternion_networks.q_RNN.QLiGRU\n",
        "        hidden_size: 64\n",
        "        bidirectional: True\n",
        "    linear: !name:speechbrain.nnet.linear.Linear\n",
        "        n_neurons: 43  # 42 phonemes + 1 blank\n",
        "        bias: False\n",
        "    softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "        apply_log: True\n",
        "        \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbAmc8ONIhH"
      },
      "source": [
        "Here, we have a very basic quaternion-valued CNN-LiGRU model that can be used to perform end-to-end CTC ASR!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vJJu--NH52",
        "outputId": "246cee02-968f-40bf-8219-bf2186f0d760"
      },
      "source": [
        "%cd speechbrain/tests/integration/neural_networks/ASR_CTC/\n",
        "!python example_asr_ctc_experiment.py hyperparams_quaternion_net.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/speechbrain/tests/integration/neural_networks/ASR_CTC\n",
            "100% 8/8 [00:04<00:00,  1.90it/s, train_loss=12.2]\n",
            "100% 2/2 [00:00<00:00,  5.63it/s]\n",
            "Epoch 0 complete\n",
            "Train loss: 12.20\n",
            "Stage.VALID loss: 4.81\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.09it/s, train_loss=7.1]\n",
            "100% 2/2 [00:00<00:00,  5.54it/s]\n",
            "Epoch 1 complete\n",
            "Train loss: 7.10\n",
            "Stage.VALID loss: 4.40\n",
            "Stage.VALID PER: 96.36\n",
            "100% 8/8 [00:03<00:00,  2.12it/s, train_loss=4.73]\n",
            "100% 2/2 [00:00<00:00,  5.56it/s]\n",
            "Epoch 2 complete\n",
            "Train loss: 4.73\n",
            "Stage.VALID loss: 4.32\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.08it/s, train_loss=3.7]\n",
            "100% 2/2 [00:00<00:00,  5.68it/s]\n",
            "Epoch 3 complete\n",
            "Train loss: 3.70\n",
            "Stage.VALID loss: 4.34\n",
            "Stage.VALID PER: 89.09\n",
            "100% 8/8 [00:03<00:00,  2.13it/s, train_loss=3.17]\n",
            "100% 2/2 [00:00<00:00,  5.66it/s]\n",
            "Epoch 4 complete\n",
            "Train loss: 3.17\n",
            "Stage.VALID loss: 4.74\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.11it/s, train_loss=2.85]\n",
            "100% 2/2 [00:00<00:00,  5.70it/s]\n",
            "Epoch 5 complete\n",
            "Train loss: 2.85\n",
            "Stage.VALID loss: 4.56\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.11it/s, train_loss=2.46]\n",
            "100% 2/2 [00:00<00:00,  5.52it/s]\n",
            "Epoch 6 complete\n",
            "Train loss: 2.46\n",
            "Stage.VALID loss: 4.35\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.11it/s, train_loss=2.11]\n",
            "100% 2/2 [00:00<00:00,  5.61it/s]\n",
            "Epoch 7 complete\n",
            "Train loss: 2.11\n",
            "Stage.VALID loss: 4.37\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.13it/s, train_loss=1.81]\n",
            "100% 2/2 [00:00<00:00,  5.65it/s]\n",
            "Epoch 8 complete\n",
            "Train loss: 1.81\n",
            "Stage.VALID loss: 4.20\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.06it/s, train_loss=1.47]\n",
            "100% 2/2 [00:00<00:00,  5.71it/s]\n",
            "Epoch 9 complete\n",
            "Train loss: 1.47\n",
            "Stage.VALID loss: 3.95\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.10it/s, train_loss=1.19]\n",
            "100% 2/2 [00:00<00:00,  5.63it/s]\n",
            "Epoch 10 complete\n",
            "Train loss: 1.19\n",
            "Stage.VALID loss: 4.12\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.10it/s, train_loss=0.93]\n",
            "100% 2/2 [00:00<00:00,  5.60it/s]\n",
            "Epoch 11 complete\n",
            "Train loss: 0.93\n",
            "Stage.VALID loss: 4.03\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.10it/s, train_loss=0.724]\n",
            "100% 2/2 [00:00<00:00,  5.61it/s]\n",
            "Epoch 12 complete\n",
            "Train loss: 0.72\n",
            "Stage.VALID loss: 4.39\n",
            "Stage.VALID PER: 90.91\n",
            "100% 8/8 [00:03<00:00,  2.11it/s, train_loss=0.604]\n",
            "100% 2/2 [00:00<00:00,  5.49it/s]\n",
            "Epoch 13 complete\n",
            "Train loss: 0.60\n",
            "Stage.VALID loss: 4.10\n",
            "Stage.VALID PER: 89.09\n",
            "100% 8/8 [00:03<00:00,  2.09it/s, train_loss=0.443]\n",
            "100% 2/2 [00:00<00:00,  5.54it/s]\n",
            "Epoch 14 complete\n",
            "Train loss: 0.44\n",
            "Stage.VALID loss: 4.53\n",
            "Stage.VALID PER: 87.27\n",
            "100% 2/2 [00:00<00:00,  5.48it/s]\n",
            "Stage.TEST loss: 4.53\n",
            "Stage.TEST PER: 87.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GaBWZd2CpUM"
      },
      "source": [
        "# **About SpeechBrain**\n",
        "- Website: https://speechbrain.github.io/\n",
        "- Code: https://github.com/speechbrain/speechbrain/\n",
        "- HuggingFace: https://huggingface.co/speechbrain/\n",
        "\n",
        "\n",
        "# **Citing SpeechBrain**\n",
        "Please, cite SpeechBrain if you use it for your research or business.\n",
        "\n",
        "```bibtex\n",
        "@misc{speechbrain,\n",
        "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
        "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
        "  year={2021},\n",
        "  eprint={2106.04624},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={eess.AS},\n",
        "  note={arXiv:2106.04624}\n",
        "}\n",
        "```"
      ]
    }
  ]
}