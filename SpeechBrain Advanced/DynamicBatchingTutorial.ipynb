{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lMT9knfou6A"
      },
      "source": [
        "# Dynamic Batching: What is it and why it is necessary sometimes\n",
        "\n",
        "Batching examples together allows us to significantly speed up training. \n",
        "This, along with distributed training across multiple GPUs allows training models with tens of millions of parameters on thousands of hours of audio data in days instead of months. \n",
        "\n",
        "The most common approach is to batch examples together using a fixed batch size.\n",
        "However, if each input has a different size (e.g. audio or features of different lengths) this requires to pad each example in each batch to match the size of the biggest one in the batch.\n",
        "\n",
        "It is easy to see that this practice leads to a waste of computing if the length of the examples has a large variance. \n",
        "In this instance, which happens in most audio and NLP applications, a large part of the computation will be done on the padded values. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILFmgtVDaJK"
      },
      "source": [
        "To illustrate this point, let's look, for example, at **MiniLibriSpeech** which is a subset of LibriSpeech. Let's download this dataset and other tools from the [data-io tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing) which uses this same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xb6KaE6DkvI"
      },
      "source": [
        "%%capture\n",
        "# here we download the material needed for this tutorial: images and an example based on mini-librispeech\n",
        "!wget https://www.dropbox.com/s/b61lo6gkpuplanq/MiniLibriSpeechTutorial.tar.gz?dl=0\n",
        "!tar -xvzf MiniLibriSpeechTutorial.tar.gz?dl=0\n",
        "# downloading mini_librispeech dev data\n",
        "!wget https://www.openslr.org/resources/31/train-clean-5.tar.gz\n",
        "!tar -xvzf train-clean-5.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgEBlx-iVht4"
      },
      "source": [
        "Next, we install `speechbrain`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVHJYKO8tOic"
      },
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6sGSbUUEitE"
      },
      "source": [
        "Now, let's look at what is the length of each audio in this dataset and how \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "it is distributed. \n",
        "\n",
        "We can plot the histogram of the lengths for each audio in this dataset using `torchaudio`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -a"
      ],
      "metadata": {
        "id": "wH03pLDuNBN9",
        "outputId": "00059750-d8ed-4c28-d9c3-50271c358ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " .\t      'MiniLibriSpeechTutorial.tar.gz?dl=0'   sbdataio.png\n",
            " ..\t       multichannel.json\t\t      train-clean-5.tar.gz\n",
            " .config       parse_data.py\n",
            " LibriSpeech   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcGLwrwEtQG",
        "outputId": "5fa4f899-b98b-4f86-92ec-12d636b22c54"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import torchaudio\n",
        "import os\n",
        "# fetching all flac files in MiniLibriSpeech\n",
        "all_flacs = glob.glob(os.path.join(\"./LibriSpeech/train-clean-5\", \"**/*.flac\"), recursive=True)\n",
        "print(\"Number of audio files in MiniLibriSpeech train-clean-5: \", len(all_flacs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files in MiniLibriSpeech train-clean-5:  1519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqdrRzYHCOf"
      },
      "source": [
        "We can see that most files have a length between 14 and 16 seconds. Moreover, there is a large variance in the file length. \n",
        "So if we sample randomly without any particular strategy a certain number of examples (e.g., 8), pad them, and batch them together we will end up with lots of padded values.\n",
        "\n",
        "This way, we will waste a significant portion of computation on padded values. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZhST3jGLgRV"
      },
      "source": [
        "We can try to effectively compute the total number of samples which belong to padding when iterating over the whole dataset with a fixed batch size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WcpKPoiDSi"
      },
      "source": [
        "We follow here SpeechBrain data preparation best practices. \n",
        "We parse all examples into a `.json` file so that parsing occurs only once and not at the start of each new experiment. In fact, parsing many small files can take a lot of time on networked storage or slow physical hard-drives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0RYowCdOHp3"
      },
      "source": [
        "# prepare LibriSpeech dataset using pre-made, downloaded parse_data.py script from \n",
        "# the data-io tutorial available here: https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing\n",
        "from parse_data import parse_to_json\n",
        "parse_to_json(\"./LibriSpeech/train-clean-5\")\n",
        "# this produced a manifest data.json file: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j--OL9sideZ"
      },
      "source": [
        "We can briefly look at each `.json` file. In particular we are interested in the `length` field which contains the length in samples for each audio in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Il1pGvhLZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d122857-2c1d-4e88-db6a-7bdb2f49ac84"
      },
      "source": [
        "!tail -n 20 data.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    },\n",
            "    \"5789-70653-0020\": {\n",
            "        \"file_path\": \"./LibriSpeech/train-clean-5/5789/70653/5789-70653-0020.flac\",\n",
            "        \"words\": \"IS IT THE FACT THAT YOUR RELATIONS WITH YOUR LATE HUSBAND HAD NOT BEEN FOR SOME TIME PAST RELATIONS OF MUTUAL AFFECTION AND CONFIDENCE\",\n",
            "        \"spkID\": \"speaker_5789\",\n",
            "        \"length\": 164320\n",
            "    },\n",
            "    \"5789-70653-0005\": {\n",
            "        \"file_path\": \"./LibriSpeech/train-clean-5/5789/70653/5789-70653-0005.flac\",\n",
            "        \"words\": \"THE IDENTITY OF THE DEAD MAN WAS PROVED BY HIS WIFE THE FIRST WITNESS CALLED FROM WHOM THE CORONER AFTER SOME INQUIRY INTO THE HEALTH AND CIRCUMSTANCES OF THE DECEASED\",\n",
            "        \"spkID\": \"speaker_5789\",\n",
            "        \"length\": 189600\n",
            "    },\n",
            "    \"5789-70653-0030\": {\n",
            "        \"file_path\": \"./LibriSpeech/train-clean-5/5789/70653/5789-70653-0030.flac\",\n",
            "        \"words\": \"HE STARTED AS HE STOOD ASIDE FROM THE DOOR WITH A SLIGHT BOW TO HEAR MISSUS MANDERSON ADDRESS HIM BY NAME IN A LOW VOICE HE FOLLOWED HER A PACE OR TWO INTO THE HALL I WANTED TO ASK YOU\",\n",
            "        \"spkID\": \"speaker_5789\",\n",
            "        \"length\": 223760\n",
            "    }\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FNKDqIeisYY"
      },
      "source": [
        "We can use this `.json` manifest file to instantiate a SpeechBrain `DynamicItemDataset` object. \n",
        "\n",
        "If this is not clear refer to the [data-io tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing). \n",
        "\n",
        "We also define a `data-io pipeline` to read the audio file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBIoa_DQhNt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b877536-d055-4bf2-8354-170982b5777e"
      },
      "source": [
        "# initializing a sb dataset object from this json\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "import speechbrain\n",
        "train_data = speechbrain.dataio.dataset.DynamicItemDataset.from_json(\"data.json\")\n",
        "# we define a pipeline to read audio\n",
        "@speechbrain.utils.data_pipeline.takes(\"file_path\")\n",
        "@speechbrain.utils.data_pipeline.provides(\"signal\")\n",
        "def audio_pipeline(file_path):\n",
        "      sig = speechbrain.dataio.dataio.read_audio(file_path)\n",
        "      return sig\n",
        "# setting the pipeline\n",
        "train_data.add_dynamic_item(audio_pipeline) \n",
        "train_data.set_output_keys([\"signal\", \"file_path\"])\n",
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'signal': tensor([0.0017, 0.0021, 0.0028,  ..., 0.0019, 0.0018, 0.0018]),\n",
              " 'file_path': './LibriSpeech/train-clean-5/6848/252323/6848-252323-0026.flac'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CsCgHaHjPj0"
      },
      "source": [
        "Voilà, we now can start to iterate over this dataset using a torch `Dataloader`. \n",
        "By using `PaddedBatch` as a `collate_fn` SpeechBrain will handle padding automatically for us. Neat! \n",
        "\n",
        "We can also define a simple function `count_samples` to count samples that belong to padding in each batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJsvc1C0h4cf"
      },
      "source": [
        "# counting tot padded values when batching the dataset with batch_size = 8\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lypSm786W1GH"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from speechbrain.dataio.batch import PaddedBatch\n",
        "# PaddedBatch will pad audios to the right\n",
        "dataloader = DataLoader(train_data, collate_fn=PaddedBatch, batch_size=batch_size)\n",
        "\n",
        "def count_samples(dataloader):\n",
        "  true_samples = 0\n",
        "  padded_samples = 0\n",
        "  t1 = time.time()\n",
        "  for batch in dataloader:\n",
        "    audio, lens = batch.signal\n",
        "    true_samples += torch.sum(audio.shape[-1]*lens).item()\n",
        "    padded_samples += torch.sum(audio.shape[-1]*(1-lens)).item()\n",
        "\n",
        "  elapsed = time.time() - t1\n",
        "  tot_samples = true_samples + padded_samples\n",
        "  return true_samples / tot_samples, padded_samples / tot_samples, elapsed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataloader:\n",
        "    print(i.signal)"
      ],
      "metadata": {
        "id": "rlfkiahZa0Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp0vH1iCjwXl"
      },
      "source": [
        "Let's count the samples when using a fixed batch size of 32 and the examples are sampled randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKOZybJ0jtQM"
      },
      "source": [
        "percent_true, percent_padded, elapsed = count_samples(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9FvJWym2TRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44b44ce-c6a1-41a3-d737-7642c8c2cd17"
      },
      "source": [
        "print(\"Random Sampling: % True samples {}, % of padding {}, Total time {}\".format(percent_true, percent_padded, elapsed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Sampling: % True samples 0.7694381956236618, % of padding 0.2305618043763382, Total time 7.405794620513916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tid6zWnK2_as"
      },
      "source": [
        "*We* are wasting more than 20% of computations in each training iteration on useless values which are only there to enable batched computations. \n",
        "\n",
        "Can we avoid such waste, speed up training, and consume less energy?\n",
        "\n",
        "Sure, we can simply sort the dataset according to the length of the examples in ascending or descending order and then batch the examples together.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-_sxv8w6hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1c92e1-ba14-4e1c-ef82-3ee1e517c53c"
      },
      "source": [
        "# if you followed the data-io tutorial you already know that sorting is super simple:\n",
        "sorted_data = train_data.filtered_sorted(sort_key=\"length\")\n",
        "dataloader = DataLoader(sorted_data, collate_fn=PaddedBatch, batch_size=batch_size)\n",
        "percent_true, percent_padded, elapsed = count_samples(dataloader)\n",
        "print(\"After sorting: % True samples {}, % of padding {}, Total time {}\".format(percent_true, percent_padded, elapsed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After sorting: % True samples 0.9877999198583547, % of padding 0.012200080141645281, Total time 7.2099609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmmBM6E_5CFh"
      },
      "source": [
        "That is quite a reduction. Now, we are almost not wasting any compute on padded values as we have minimized padding by taking audios with roughly the same length in each batch. Iterating over one epoch is also significantly faster.\n",
        "\n",
        "But this means that we must train with a sorted dataset. \n",
        "In some applications, this might hurt the performance as the network sees the examples always in the same order. \n",
        "\n",
        "In other applications sorting the examples can instead bring better performance as it can be seen as a sort of curriculum learning. This is the case for example for our TIMIT recipes. \n",
        "\n",
        "Dynamic Batching allows users to trade-off between full random sampling of the examples and deterministic sampling from sorted examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOxnX03A1eS6"
      },
      "source": [
        "Another problem with fixed batch size is that we are under-utilizing our resources for the shortest examples.\n",
        "Suppose we use a fixed batch size of 8, and our dataset is sorted in ascending order. This means we must have sufficient memory to train on the 8 longest examples. But we also train on the 8 shortest ones! \n",
        "In many instances, we can afford to batch a larger number of shorter examples together and optimize the GPU usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEuEnbEz75sr"
      },
      "source": [
        "# SpeechBrain `DynamicBatchSampler` class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mecsawr--Fff"
      },
      "source": [
        "SpeechBrain provides a useful abstraction to perform Dynamic Batching: \n",
        "\n",
        "---\n",
        "\n",
        "**DynamicBatchSampler**. \n",
        "\n",
        "In particular, with the right settings, it allows us to train large models even with 12 GB VRAM GPUs in a reasonable time. When using high-performance high VRAM GPUs, instead, it can significantly reduce training time. \n",
        "\n",
        "**This abstraction allows us to select a good trade-off between training speed, randomization of sampling, and VRAM usage.**\n",
        "\n",
        "It is up to you, depending on your application scenario and hardware, which of these characteristics should be prioritized. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8M-LuNBobTk"
      },
      "source": [
        "`DynamicBatchSampler` belongs to the `torch.utils.data` `Sampler` class and is a torch *Batch Sampler*: \n",
        "\n",
        "Being a batch Sampler, it is just a *python generator*  which returns, at each call, a list containing the indexes of the examples which should be batched together by the `DataLoader` using the `collate_fn`. These indexes are used to fetch the actual examples in the `torch.utils.data.Dataset` class using the `__getitem__` method. \n",
        "\n",
        "Here is an example with batch_size 2. The DataLoader is responsible for taking care of parallelization of the Dataset `__getitem__` method. The indexes of the examples are provided by the Batch Sampler. \n",
        "For more info, you can refer to the official [Pytorch documentation on torch.utils.data](https://pytorch.org/docs/stable/data.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaRTseKUgz5D"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1bNaa6X_Rjnt7qyFiPf4Hz0Gi6XLh8fAw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4OQaEM5kQMD"
      },
      "source": [
        "## Using `speechbrain.dataio.samplers.DynamicBatchSampler`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFzGEwfJkZxx"
      },
      "source": [
        "`DynamicBatchSampler` has several input arguments upon instantiation and provides a great deal of flexibility.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "We will practically illustrate what is the effect of some of these using MiniLibriSpeech and how each of these can change the trade-off between speed, randomization, and VRAM usage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tM5iEaYolWS"
      },
      "source": [
        "**NOTE:** you should be highly familiar with SpeechBrain [data-io](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing) to follow this tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTogAfnOrEjp"
      },
      "source": [
        "# initializing a sb dataset object from this json\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "import speechbrain\n",
        "# we instantiate here the train data dataset from the json manifest file \n",
        "train_data = DynamicItemDataset.from_json(\"data.json\")\n",
        "# we define a pipeline to read audio\n",
        "@speechbrain.utils.data_pipeline.takes(\"file_path\")\n",
        "@speechbrain.utils.data_pipeline.provides(\"signal\")\n",
        "def audio_pipeline(file_path):\n",
        "      sig = speechbrain.dataio.dataio.read_audio(file_path)\n",
        "      return sig\n",
        "# setting the pipeline\n",
        "train_data.add_dynamic_item(audio_pipeline) \n",
        "train_data.set_output_keys([\"signal\", \"file_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggol_VzvrwME"
      },
      "source": [
        "Crucially to use `DynamicBatchSampler` **it is important that the manifest/dataset description file** (`json` or `csv`) **contains**, for each example, **an entry which specifies the duration or length of each example**. \n",
        "The `DynamicBatchSampler` will use this information to batch efficiently examples together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3JSZOecrzud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3753f03-65a6-4e8f-c661-65e6705867f2"
      },
      "source": [
        "!tail -n 10 data.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        \"spkID\": \"speaker_669\",\n",
            "        \"length\": 174160\n",
            "    },\n",
            "    \"669-129074-0008\": {\n",
            "        \"file_path\": \"./LibriSpeech/train-clean-5/669/129074/669-129074-0008.flac\",\n",
            "        \"words\": \"THERE HEARD OF HER FASCINATIONS AND WERE QUITE CURIOUS TO KNOW HER WHEN IT BECAME KNOWN THAT SHE WAS NOBLE OF AN ANCIENT ENGLISH FAMILY THAT HER HUSBAND WAS A COLONEL OF THE GUARD\",\n",
            "        \"spkID\": \"speaker_669\",\n",
            "        \"length\": 256400\n",
            "    }\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9 // 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn4d3XXP3OSS",
        "outputId": "4881dea5-f6d9-4ed0-877f-ce67cce9a934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elu2P2lar5lH"
      },
      "source": [
        "We can see that in this case we have a length key containing, for each audio, the length in samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEPu-d6q8r-"
      },
      "source": [
        "### Instantiating `DynamicBatchSampler`: Core Parameters\n",
        "\n",
        "---\n",
        "At its core, `DynamicBatchSampler` batches examples with similar lengths based on \"buckets\". Upon instantiation, based on the input args, several buckets are created. These buckets define a number of contiguous intervals e.g. $0\\leq x < 200, 200 \\leq x < 400$ and so on.  \n",
        "Examples whose lengths fall into a certain bucket are assumed as they have the same length and can be batched together. In some way, we are \"quantizing\" the lengths of the examples in the dataset. \n",
        "\n",
        "In the Figure below we have N buckets, each defined by his right boundary. \n",
        "For each bucket, we can have a different `batch_size` because we can fit more examples falling in the leftmost bucket than the rightmost one. \n",
        "\n",
        "For the first bucket, the batch size is 8 because 1725 // 200 = 8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XilSLmHmtHYY"
      },
      "source": [
        "<img width=\"600px\" src=\"https://raw.githubusercontent.com/vitas-ai/tutorial-dynamic-batching-images/main/ManualBuckets-3.png\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tQSe564wK2U"
      },
      "source": [
        "In the Figure below we illustrate how 14 examples with different lengths are \"bucketized\": 4 examples in the first bucket, 5 examples in the second, 2 in the third, 2 in the fourth and one in the last. \n",
        "\n",
        "One example is discarded because it is too long (its length is more than `max_batch_size`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXmT-3KWwIkV"
      },
      "source": [
        "<img width=\"600px\" src=\"https://raw.githubusercontent.com/vitas-ai/tutorial-dynamic-batching-images/main/ManualBuckets-4.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNco4lz-JurP"
      },
      "source": [
        "A minimal instantiation of `DynamicBatchSampler` requires four arguments at least: \n",
        "\n",
        "1.   A `Dataset` object (`train_data` here, note it can also be validation or test set). \n",
        "2.   `max_batch_length`: the maximum length we want in a batch. This will be the maximum aggregated length of all examples in a batch we are going to allow and must be chosen carefully to avoid OOM errors. \n",
        "A higher number means we are going to have, on average, an higher batch size so you must apply the same \"tricks\" as when batch size is increased for standard fixed batch size training. E.g. increase learning rate. \n",
        "3. `num_buckets`: number of buckets one wishes to use. If just one bucket is used, all examples can be batched together, and dynamic batching in this instance is the same as uniform random sampling of the examples. \n",
        "If too many buckets are specified the training will be slow because some buckets will be half empty. \n",
        "As a rule of thumb: num_buckets trades-off speed with randomization.\n",
        "Low number -> better randomization, High number -> faster training.\n",
        "\n",
        "4. `length_func`: function to be applied to each dataset element to get its length. In our case, we can see that the `.json` manifest contains a key *length* which specifies each audio length in samples. This can be used for example to convert the length into seconds or the number of feature frames. So that `max_batch_length` and the bucket boundaries will be specified not anymore in samples. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify `max_batch_length` in terms of seconds "
      ],
      "metadata": {
        "id": "xmNzc38_Hfow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.dataio.sampler import DynamicBatchSampler\n",
        "\n",
        "max_batch_len = 17*32 \n",
        "\n",
        "dynamic_batcher = DynamicBatchSampler(\n",
        "    train_data,\n",
        "    max_batch_length=max_batch_len,\n",
        "    num_buckets=60,\n",
        "    length_func=lambda x: x[\"length\"] / 16000,\n",
        ")"
      ],
      "metadata": {
        "id": "cEk1NgP9HjwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_batcher._ex_lengths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "I-KXWrvq_ZuO",
        "outputId": "ede3c8d7-ef58-486c-8e3c-8075e599e63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-80c4b4500311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdynamic_batcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ex_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dynamic_batcher' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dynamic_batcher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjLGSB-IhD0K",
        "outputId": "273c263b-d47f-46d1-adb6-1806e1729888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in dynamic_batcher:\n",
        "  print(len(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwDemOO-U5G9",
        "outputId": "f6c45d6f-e940-4632-c6c3-96f9b68a30c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "38\n",
            "34\n",
            "38\n",
            "44\n",
            "34\n",
            "71\n",
            "57\n",
            "34\n",
            "34\n",
            "53\n",
            "34\n",
            "34\n",
            "34\n",
            "38\n",
            "17\n",
            "34\n",
            "71\n",
            "34\n",
            "16\n",
            "38\n",
            "44\n",
            "35\n",
            "34\n",
            "34\n",
            "30\n",
            "38\n",
            "34\n",
            "53\n",
            "8\n",
            "38\n",
            "34\n",
            "44\n",
            "30\n",
            "38\n",
            "38\n",
            "26\n",
            "34\n",
            "34\n",
            "38\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in dynamic_batcher:\n",
        "    print(sum([train_data[i]['signal'].shape[0]/16000 for i in b]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhlzdqqNUKTL",
        "outputId": "d9b15c09-79d5-4839-ac10-74812daabeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508.36500000000007\n",
            "475.06006250000013\n",
            "504.4548749999999\n",
            "322.46993750000007\n",
            "508.49\n",
            "514.4699999999999\n",
            "514.1000624999999\n",
            "513.0549999999998\n",
            "520.12\n",
            "507.77506250000005\n",
            "513.8399374999999\n",
            "195.82999999999996\n",
            "72.53\n",
            "516.8400000000001\n",
            "502.595125\n",
            "511.2699375\n",
            "484.9699375\n",
            "513.4449999999999\n",
            "278.46000000000004\n",
            "504.04993749999994\n",
            "514.21\n",
            "343.69993750000003\n",
            "512.62\n",
            "511.53999999999996\n",
            "509.32999999999987\n",
            "468.21000000000004\n",
            "503.8549999999999\n",
            "513.83\n",
            "489.04\n",
            "424.315\n",
            "516.3600000000001\n",
            "515.8849999999999\n",
            "243.755\n",
            "519.3149375\n",
            "507.1750000000001\n",
            "503.085\n",
            "512.555\n",
            "509.4100000000001\n",
            "514.035\n",
            "491.8999999999999\n",
            "512.3600625000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfewXTPxuHI9"
      },
      "source": [
        "### Using `DynamicBatchSampler`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pMNyqs-vvAj"
      },
      "source": [
        "Once this special batch sampler is instantiated it can be used in the standard Pytorch way by using it as a DataLoader argument: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwyIQz5ROaw5"
      },
      "source": [
        "dataloader = DataLoader(train_data, batch_sampler=dynamic_batcher, collate_fn=PaddedBatch)\n",
        "# note that the batch size in the DataLoader cannot be specified when a batch sampler is used.\n",
        "# the batch size is handled by the batch_sampler and in this case is dynamic "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YWQI8bUwN2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5cb36b-e93f-4b65-b8a9-72e9236af1e1"
      },
      "source": [
        "# we can iterate now over the data in an efficient way using dynamic batching.\n",
        "# our DynamicBatchSampler will sample the index of the examples such that padding is minimized\n",
        "# while PaddedBatch will handle the actual padding and batching.\n",
        "# everything happens in parallel thanks to the torch DataLoader.\n",
        "first_batch = next(iter(dataloader))\n",
        "print(first_batch.signal.lengths.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([34])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXOYNdw4wvD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e240886-276a-4327-c4ee-2950fd8e37a0"
      },
      "source": [
        "first_batch.signal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PaddedData(data=tensor([[ 0.0022,  0.0005,  0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0004,  0.0007,  0.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0010,  0.0005,  0.0003,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [ 0.0005,  0.0004,  0.0005,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0002, -0.0009, -0.0009,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0008,  0.0000, -0.0006,  ...,  0.0000,  0.0000,  0.0000]]), lengths=tensor([0.9894, 0.9349, 0.9925, 0.9712, 0.9687, 0.9036, 0.9377, 0.9327, 0.9568,\n",
              "        0.9136, 0.9803, 0.9768, 0.9308, 0.9274, 0.9806, 0.9546, 0.8957, 0.9599,\n",
              "        0.9415, 0.9173, 1.0000, 0.9837, 0.9449, 0.9599, 0.9887, 0.9264, 0.9900,\n",
              "        0.9934, 0.8923, 0.9512, 0.9145, 0.9483, 0.9878, 0.9411]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY0AwDiSxdVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce048be3-0960-4625-c9c0-d749ce879ea2"
      },
      "source": [
        "percent_true, percent_padded, elapsed = count_samples(dataloader)\n",
        "print(\"With Dynamic Batching: % True samples {}, % of padding {}, Total time {}\".format(percent_true, percent_padded, elapsed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Dynamic Batching: % True samples 0.9206615837444373, % of padding 0.07933841625556273, Total time 7.263784408569336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AuQKRKIxv-j"
      },
      "source": [
        "**The amount of padded values is significantly reduced vs the fixed batch size and full uniform random sampling.** \n",
        "\n",
        "It indeed is close to what is obtained with fully deterministic sorting and fixed batch size. \n",
        "The difference is that, here, with the DynamiBatchSampler we can still allow for some randomness in the sampling strategy.\n",
        "\n",
        "Moreover, by batching together examples changing the batch size we use our hardware at the fullest with each batch significantly speeding up training. \n",
        "\n",
        "We can look at the maximum number of examples that are batched together:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dynamic_batcher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zjPhvgoZihU",
        "outputId": "774c4ce6-b6e0-4d8b-a29d-d21e107ba5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the DynamicBatchSampler with the current parameters we have 41 batches. \n",
        "\n",
        "While using a fixed batch size of 32 we would end up with:"
      ],
      "metadata": {
        "id": "8guqeYr1Z9_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data) // 32 + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiO7KoM0Z1mi",
        "outputId": "f1a77ccf-12da-45f9-e927-3a7c197d80f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "so more training iterations, with more padded values --> longer training time. "
      ],
      "metadata": {
        "id": "1djPRPX7aKEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GsxhAhmdTA3h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhMuUFeuSgx"
      },
      "source": [
        "Another way to use `DynamicBatchSampler` straightforwardly is by feeding it directly to the Brain class as an additional argument via `run_opts`. In this case, the Brain class will implicitly instantiate for you a `DataLoader`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ig9zJi0ugjn"
      },
      "source": [
        "## dummy Brain class here with dummy model \n",
        "class SimpleBrain(speechbrain.Brain):\n",
        "  def compute_forward(self, batch, stage):\n",
        "    return model(batch[\"signal\"][0].unsqueeze(1))\n",
        "  def compute_objectives(self, predictions, batch, stage):\n",
        "    loss_dummy = torch.mean(predictions)\n",
        "    return loss_dummy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HdYfcSO_v1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650b8b76-7529-4fc3-9201-9b16229a917a"
      },
      "source": [
        "zmodel = torch.nn.Conv1d(1, 1, 3)\n",
        "brain = SimpleBrain({\"model\": model}, opt_class=lambda x: torch.optim.SGD(x, 0.1), run_opts={\"batch_sampler\": dynamic_batcher})\n",
        "brain.fit(range(1), train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1519/1519 [01:17<00:00, 19.48it/s, train_loss=-76.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrsjDadz0AP_"
      },
      "source": [
        "### Advanced Parameters: Full control over randomness, training speed, and VRAM consumption. \n",
        "---\n",
        "Right now we have explored the most basilar input args for `DynamicBatchSampler`.\n",
        "Let's see more advanced parameters.\n",
        "\n",
        "#### Controlling Randomness\n",
        "\n",
        "\n",
        "Randomness in `DynamicBatchSampler` is controlled with `shuffle` and `batch_ordering`.\n",
        "\n",
        "`shuffle` is a flag:\n",
        "\n",
        "* if `true`, then dynamic batches are created based on random sampling (deterministically based on `epoch` and `seed` parameters) at each epoch (included upon `DynamicBatchSampler` instantiation or epoch 0);\n",
        "* if `false`, then dynamic batches are created taking the examples from the database as they are. If the dataset is sorted in ascending or descending order this ordering is preserved. Note that if `false` the batches will be created once and never change during training (their permutation can change however see next).\n",
        "\n",
        "\n",
        "\n",
        "Batch permutation depends on `batch_ordering`:\n",
        "\n",
        "* `\"random\"` deterministically shuffles batches based on `epoch` and `seed` parameters\n",
        "* `\"ascending\"` and `\"descending\"` sort the batches based on the duration of the longest example in the batch.\n",
        "\n",
        "This argument is independent of `shuffle`.`shuffle` controls if we have to shuffle the examples before creating the batches. `batch_ordering` instead controls the shuffling of the batches after they have been created.\n",
        "For example, if set to `\"ascending\"` the first batch returned by the batch sampler will be the one with the shortest example in the dataset (examples belonging to the leftmost bucket); while the last one will contain the longest example in the dataset. \n",
        "\n",
        "\n",
        "NOTE: when iterating the `DynamicBatchSampler` (calling its `__iter__` function):\n",
        "\n",
        "* dynamic batches are re-generated at each epoch if `shuffle == True`; or\n",
        "* dynamic batches are permuted at each epoch if `batch_ordering == \"random\"`\n",
        "\n",
        " \n",
        "\n",
        "Note that also `num_buckets` affects randomization of training. As we stated before if `num_buckets`-->1 we obtain full random sampling as all examples can be batched together at least if `shuffle` is True and `batch_ordering` is random. Curiously even if `num_buckets` is very large we also obtain full random sampling if `shuffle` is True and `batch_ordering` is random as practically every example in the dataset is batched alone (we will have closer to batch size == 1 and very slow training, probably you want to avoid this)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCH1TWphhmA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create the batches by firstly shuffling the examples (so the batches will be different at each epoch) but then sort them so always the one with the shortest example comes first. "
      ],
      "metadata": {
        "id": "2GR81Tjmeqxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.dataio.sampler import DynamicBatchSampler\n",
        "\n",
        "max_batch_len = 17*32 \n",
        "\n",
        "dynamic_batcher = DynamicBatchSampler(train_data,\n",
        "        max_batch_length=max_batch_len,\n",
        "        num_buckets= 60,\n",
        "        length_func=lambda x: x[\"length\"] / 16000,\n",
        "        shuffle=True,\n",
        "        batch_ordering=\"ascending\"\n",
        "        )"
      ],
      "metadata": {
        "id": "A1brcnzleTp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(train_data, batch_sampler=dynamic_batcher, collate_fn=PaddedBatch)"
      ],
      "metadata": {
        "id": "m6WQ3yOfee7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "jcR6wsWmeoKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch.signal[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoPzgwapeh4e",
        "outputId": "d592b6ff-c986-4fdd-a432-cd199d9c72bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([71, 120480])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use instead descending order "
      ],
      "metadata": {
        "id": "A07LSqX7e9PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.dataio.sampler import DynamicBatchSampler\n",
        "\n",
        "max_batch_len = 17*32 \n",
        "\n",
        "dynamic_batcher = DynamicBatchSampler(train_data,\n",
        "        max_batch_length=max_batch_len,\n",
        "        num_buckets= 60,\n",
        "        length_func=lambda x: x[\"length\"] / 16000,\n",
        "        shuffle=True,\n",
        "        batch_ordering=\"descending\"\n",
        "        )"
      ],
      "metadata": {
        "id": "ONwRup3NfAzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(train_data, batch_sampler=dynamic_batcher, collate_fn=PaddedBatch)"
      ],
      "metadata": {
        "id": "ghqqqzi4fCWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "bvWyyNEQfEBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch.signal[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsAEtRTffE8G",
        "outputId": "90d88956-cbe3-4fe0-e270-e811c3d1e0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([26, 276400])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it now returns the batch with longest example. "
      ],
      "metadata": {
        "id": "XqAasFaEfIrQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwXc_DTLJ4t0"
      },
      "source": [
        "#### Specifying manually the buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXTgObmHJWYa"
      },
      "source": [
        "The argument `bucket_boundaries` can be used to manually specify how many buckets and what are their boundaries. \n",
        "\n",
        "Needlessy to say, this arg will supersed`num_buckets`.\n",
        "\n",
        "Let's see an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npoZZCNaKZkO"
      },
      "source": [
        "# trivial example just one bucket \n",
        "dynamic_batcher = DynamicBatchSampler(train_data,\n",
        "        max_batch_length=max_batch_len,\n",
        "        bucket_boundaries=[max_batch_len],\n",
        "        length_func=lambda x: x[\"length\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9mYhtGiSH5q"
      },
      "source": [
        "It is easy to see that having just one bucket in this case all examples can be batched together. Even the shortest ones with the longest ones. \n",
        "\n",
        "When just one bucket is used the `DynamicBatchSampler` will be inefficient as it will not minimize at all the amount of padding in each batch with a behavior similar to having a fixed batch size. \n",
        "\n",
        "As we said previously we have the maximal amount of randomness in each batch as each example can be batched with any other one, regardless of its length. \n",
        "We can now see more clearly the trade-off between training speed and randomness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pYvO6cqTVTz"
      },
      "source": [
        "Here, in a more practical example, we use `bucket_boundaries` argument to specify a distribution for the buckets, given the distribution of the length of the audio files in our dataset, which we have plotted before and has, a **reversed log-normal distribution**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaykn4vlUFLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da72802e-47b1-46a8-a2d5-040657f6c5e6"
      },
      "source": [
        "# number of buckets --> less buckets more randomness\n",
        "n_buckets = 40\n",
        "# we can create n_buckets linearly spaced\n",
        "max_batch_len = 20000\n",
        "import numpy as np\n",
        "buckets = np.linspace(0, max_batch_len, n_buckets)\n",
        "buckets_bounds = buckets[1:].tolist()\n",
        "dynamic_batcher = DynamicBatchSampler(train_data,\n",
        "        max_batch_length=max_batch_len,\n",
        "        bucket_boundaries=buckets_bounds,\n",
        "        length_func=lambda x: x[\"length\"] / 160)# length in terms of 10ms\n",
        "\n",
        "dataloader = DataLoader(train_data, batch_sampler=dynamic_batcher, collate_fn=PaddedBatch)\n",
        "percent_true, percent_padded, elapsed = count_samples(dataloader)\n",
        "print(\"With Dynamic Batching: % True samples {}, % of padding {}, Total time {}\".format(percent_true, percent_padded, elapsed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Dynamic Batching: % True samples 0.8991611894999626, % of padding 0.10083881050003737, Total time 7.179309606552124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YPaFBk6EfN65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_batch_len = 20000\n",
        "n_buckets = 40\n",
        "buckets = np.linspace(0, max_batch_len, n_buckets)\n",
        "buckets[1:].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjY18IaiuLve",
        "outputId": "cf4e673f-6e73-4d89-98d3-e0a9eb60fe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[512.8205128205128,\n",
              " 1025.6410256410256,\n",
              " 1538.4615384615386,\n",
              " 2051.2820512820513,\n",
              " 2564.102564102564,\n",
              " 3076.923076923077,\n",
              " 3589.74358974359,\n",
              " 4102.5641025641025,\n",
              " 4615.384615384615,\n",
              " 5128.205128205128,\n",
              " 5641.025641025641,\n",
              " 6153.846153846154,\n",
              " 6666.666666666667,\n",
              " 7179.48717948718,\n",
              " 7692.307692307692,\n",
              " 8205.128205128205,\n",
              " 8717.948717948719,\n",
              " 9230.76923076923,\n",
              " 9743.589743589744,\n",
              " 10256.410256410256,\n",
              " 10769.23076923077,\n",
              " 11282.051282051281,\n",
              " 11794.871794871795,\n",
              " 12307.692307692309,\n",
              " 12820.51282051282,\n",
              " 13333.333333333334,\n",
              " 13846.153846153846,\n",
              " 14358.97435897436,\n",
              " 14871.794871794871,\n",
              " 15384.615384615385,\n",
              " 15897.435897435897,\n",
              " 16410.25641025641,\n",
              " 16923.076923076922,\n",
              " 17435.897435897437,\n",
              " 17948.71794871795,\n",
              " 18461.53846153846,\n",
              " 18974.358974358973,\n",
              " 19487.17948717949,\n",
              " 20000.0]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYwDqcDW4GE"
      },
      "source": [
        "*However*, having linearly spaced buckets when our length distribution is not uniform is sub-optimal. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuitively one better way to generate the buckets is using an exponential distribution as we can employ coarser buckets for longer examples.\n",
        "Indeed, more padding for longer examples has less impact as overall the examples are longer. "
      ],
      "metadata": {
        "id": "0dJX0aJvp1zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of buckets --> less buckets more randomness\n",
        "n_buckets = 40\n",
        "# we can create n_buckets linearly spaced\n",
        "max_batch_len = 20000\n",
        "import numpy as np\n",
        "batch_multiplier = 1.2\n",
        "buckets_bounds = [200]\n",
        "for x in range(n_buckets):\n",
        "  buckets_bounds.append(buckets_bounds[-1]*batch_multiplier)\n",
        "\n",
        "dynamic_batcher = DynamicBatchSampler(train_data,\n",
        "        max_batch_length=max_batch_len,\n",
        "        bucket_boundaries=buckets_bounds,\n",
        "        length_func=lambda x: x[\"length\"] / 160) # length in terms of 10ms\n",
        "\n",
        "dataloader = DataLoader(train_data, batch_sampler=dynamic_batcher, collate_fn=PaddedBatch)\n",
        "percent_true, percent_padded, elapsed = count_samples(dataloader)\n",
        "print(\"With Dynamic Batching: % True samples {}, % of padding {}, Total time {}\".format(percent_true, percent_padded, elapsed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rnF21yhqjUF",
        "outputId": "e9e95c53-146a-4794-ce3a-a68a310a2aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Dynamic Batching: % True samples 0.9405700081022755, % of padding 0.05942999189772446, Total time 7.119041681289673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of buckets --> less buckets more randomness\n",
        "n_buckets = 40\n",
        "# we can create n_buckets linearly spaced\n",
        "max_batch_len = 20000\n",
        "import numpy as np\n",
        "batch_multiplier = 1.2\n",
        "buckets_bounds = [200]\n",
        "for x in range(n_buckets):\n",
        "  buckets_bounds.append(buckets_bounds[-1]*batch_multiplier)"
      ],
      "metadata": {
        "id": "kWgXH7tVfnO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buckets_bounds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgVKsSO2fowv",
        "outputId": "6fdb7644-1c28-45ae-b3c7-904a5baca095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[200,\n",
              " 240.0,\n",
              " 288.0,\n",
              " 345.59999999999997,\n",
              " 414.71999999999997,\n",
              " 497.66399999999993,\n",
              " 597.1967999999999,\n",
              " 716.6361599999999,\n",
              " 859.9633919999999,\n",
              " 1031.9560703999998,\n",
              " 1238.3472844799996,\n",
              " 1486.0167413759996,\n",
              " 1783.2200896511995,\n",
              " 2139.8641075814394,\n",
              " 2567.836929097727,\n",
              " 3081.4043149172726,\n",
              " 3697.685177900727,\n",
              " 4437.222213480873,\n",
              " 5324.666656177047,\n",
              " 6389.599987412456,\n",
              " 7667.519984894947,\n",
              " 9201.023981873936,\n",
              " 11041.228778248722,\n",
              " 13249.474533898467,\n",
              " 15899.36944067816,\n",
              " 19079.24332881379,\n",
              " 22895.09199457655,\n",
              " 27474.110393491857,\n",
              " 32968.93247219023,\n",
              " 39562.71896662827,\n",
              " 47475.26275995393,\n",
              " 56970.31531194471,\n",
              " 68364.37837433365,\n",
              " 82037.25404920038,\n",
              " 98444.70485904044,\n",
              " 118133.64583084853,\n",
              " 141760.37499701823,\n",
              " 170112.44999642187,\n",
              " 204134.93999570623,\n",
              " 244961.92799484747,\n",
              " 293954.31359381694]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dynamic_batcher._batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioyIneckuE1_",
        "outputId": "78f20a2f-36d7-4f5b-82a5-facc541274d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The amount of padding is reduced by using a more appropriate distribution. \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cgRNd2latc2a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtHZft5Vg010"
      },
      "source": [
        "lengths = np.array([torchaudio.info(x).num_frames for x in all_flacs])\n",
        "from scipy.stats import beta\n",
        "lengths = (lengths - np.amin(lengths)) / (np.amax(lengths)- np.amin(lengths))\n",
        "lengths = np.clip(lengths, 1e-6, 1-1e-6)\n",
        "a, b, loc, upper = beta.fit(lengths, floc=0, fscale=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHAEaTMc9rg7"
      },
      "source": [
        "## How to find good hyper-parameters and speed up training with DynamicBatchSampler\n",
        "\n",
        "\n",
        "Training speed largely depends on:\n",
        "\n",
        "\n",
        "*   `max_batch_length`: you want to set this as high as possible without getting OOM errors.\n",
        "*   `num_buckets`: you want to avoid too low values and too high values for this parameter. As said previously: too low values and shorter examples will be batched also with longer ones, too high and almost all examples are batched alone. In both cases, your training will be extremely slow. \n",
        "\n",
        "\n",
        "Finding a good value for  `max_batch_length`:\n",
        "\n",
        "\n",
        "1.   Sort the dataset in descending order, set `shuffle = False` and `batch_ordering = \"descending\"` and do multiple short runs increasing  `max_batch_length` till you get an OOM error. Choose a value slightly below the one that leads to OOM. \n",
        "\n",
        "Finding a good value for  `num_buckets`:\n",
        "\n",
        "1. Without using `DynamicBatchSampler`, sort the dataset in descending order and find the maximum batch size that your GPU can handle. Look at the estimated time and number of batches for this configuration given in the very first iterations. \n",
        "2. Sort the dataset in descending order, set `shuffle = False` and `batch_ordering = \"descending\"`  and `max_batch_length` with the value found before. Start with a `num_buckets` between 10 and 20 and do some guesses by doing some short runs looking at the estimated time and number of batches for each configuration. Choose the value which gives fewer batches than the one in step 1 (without dynamic batching) and whose estimated time is lower. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Batching with Web dataset\n",
        "When working on an HPC cluster it is crucial to copy the dataset to the SSD of the local computing node. This step significantly improves the data-io performance and avoids slowing down a shared filesystem. In some cases, the dataset could be too big that might not fit into the SSD. This scenario is getting more common these days with the adoption of larger and larger datasets.\n",
        "\n",
        "SpeechBrain supports [Webdataset](https://github.com/webdataset/webdataset), which allows users to efficiently read datasets from the shared file system.\n",
        "The proposed Webdataset-based solution also supports dynamic batching. For more information, please take a look at [this tutorial](https://colab.research.google.com/drive/1s171JSA53_ktvc1zQp6uMcM0TChtCcZ9?usp=sharing)."
      ],
      "metadata": {
        "id": "gVJiv2igqSyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgements\n",
        "\n",
        "SpeechBrain DynamicBatchSampler has been developed by Ralf Leibold and Andreas Nautsch with the help of Samuele Cornell"
      ],
      "metadata": {
        "id": "B3jUpS4pxv3q"
      }
    }
  ]
}